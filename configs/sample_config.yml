# sample configuration file

# constants: don't change after setup
TRAIN_DATA_DIR: "<data-folder-prefix>/train-clean-360" 
DEV_DATA_DIR: "<data-folder-prefix>/dev-clean" 
TEST_DATA_DIR: "<data-folder-prefix>/test-clean/mfcc" 
OUTPUT_DIR: "experiments"
SEED: 11785
# sample configuration file


# FOR TRAINING
# whether to use wandb for logging
wandb:
  use: false
  configs:
    project: dl3
    reinit: true
    entity: <your-wandb-id>

# whether to save a plot of lr schedule in output folder
save_lr_fig: true

# whether to keep <SOS> and <EOS>
keep_seq_tags: false

# whether to use mixed precision
use_mixed_precision: true

# number of cpu workers
num_workers: 10

# noise added to input data
noise_level: 0.5

# whether to finetune a checkpoint
finetune:
  use: true
  checkpoint: experiments/<exp-run-id>/<min_dist/min_loss/last>.pt
  relaunch_lr: false

# model architecture
model: 
  choice: one-for-all
  configs:
    init_time_mask: true
    init_freq_mask: true
    emb_time_mask: true
    emb_freq_mask: true
    feat_ext_cfgs:
      # input MFCC dimension
      dim_in: 15
      # output channels
      dims_out: 
        - 1024
      # kernel_sizes (first one for downsampling)
      kernels: 
        - 7
        - 5
      # strides (also, first for downsampling)
      strides:
        - 2
        - 2
      # whether to use layernorm in convnext block
      useLayerNorm: false
      # whether to use ConvNext blocks
      useConvNext: false

    lstm_cfgs:
      # don't specify dim_in: taken from feat_ext's dims_out[-1]
      hidden_dims:
        - 512
      num_layers:
        - 2
      bidirectionals:
        - true
      dropouts:
        - 0.20
      use_lock_dropout: true
      use_residuals:
        - false

    cls_cfgs:
      # don't specify dim_in: taken automatically from 
      #     lstm's hidden_dims[-1] & bidirectionals[-1]
      dims:
        - 4096
      # don't specify num_labels either: inferred from keep_seq_tags

# batch sizes
batch_size: 64

# number of training epochs
epochs: 50
comp_dist_int: 1

# beam search CTC decoder
decoder_configs:
  beam_width: 20
  # corresponds to [SIL]
  blank_id: 0

# scheduler: cosine annealing w/ warmups only
scheduler_manual:
  use: false
  configs:
    min_lr: 0.00001
    stages: 3
    warmup_epochs: 0
  

# pytorch scheduler
scheduler:
  use: true
  # # sample configs for MultiStepLR
  # choice: multi-step
  # configs:
  #   milestones:
  #     - 8
  #     - 12
  #     - 16
  #   gamma: 0.5
  # sample configs for ReduceLROnPlateau
  choice: reduce-on-plateau
  configs:
    mode: min
    factor: 0.5
    patience: 3
    threshold: 0.1
    threshold_mode: abs
  # for reduce-on-plateau only: track dist or loss
  use_dist_plateau: true
  
# optimizer
optimizer:
  name: adamw
  configs:
    lr: 0.0005
    weight_decay: 0.10
    lr: 0.001
    weight_decay: 0.10


# FOR INFERENCE
exp_folder: experiments/<exp-run-id>

test_decoder_configs:
  beam_width: 50
  # corresponds to [SIL]
  blank_id: 0

use_min_loss: true
use_min_dist: true
use_last: true
