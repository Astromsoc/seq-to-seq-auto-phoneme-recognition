# sample configuration file

# constants: don't change after setup
TRAIN_DATA_DIR: "<dir-prefix>/train-clean-360" 
DEV_DATA_DIR: "<dir-prefix>/dev-clean" 
TEST_DATA_DIR: "<dir-prefix>/test-clean/mfcc"
OUTPUT_DIR: "experiments"
SEED: 11785


# FOR TRAINING
# whether to use wandb for logging
wandb:
  use: true
  configs:
    project: "dl3"
    reinit: true
    entity: ""

# whether to save a plot of lr schedule in output folder
save_lr_fig: true

# whether to keep <SOS> and <EOS>
keep_seq_tags: false

# whether to use mixed precision
use_mixed_precision: true

# number of cpu workers
num_workers: 10

# whether to finetune a checkpoint
finetune:
  use: false
  checkpoint: "experiments/<checkpoint>"

# model architecture
model: 
  choice: one-for-all
  configs:
    init_time_mask: true
    init_freq_mask: true
    emb_time_mask: true
    emb_freq_mask: true
    feat_ext_cfgs:
      # input MFCC dimension
      dim_in: 15
      # output channels
      dims_out: 
        - 256
      # kernel_sizes (first one for downsampling)
      kernels: 
        - 7
        - 5
      # strides (also, first for downsampling)
      strides:
        - 2
        - 2
      # whether to use layernorm in convnext block
      useLayerNorm: false
      # whether to use ConvNext blocks
      useConvNext: false

    lstm_cfgs:
      # don't specify dim_in: taken from feat_ext's dims_out[-1]
      hidden_dims:
        - 256
      num_layers:
        - 3
      bidirectionals:
        - true
      dropouts:
        - 0.2
      useLockDropout: false

    cls_cfgs:
      # don't specify dim_in: taken automatically from 
      #     lstm's hidden_dims[-1] & bidirectionals[-1]
      dims:
        - 1024
      # don't specify num_labels either: inferred from keep_seq_tags

# batch sizes
batch_size: 256

# number of training epochs
epochs: 40
comp_dist_int: 1

# beam search CTC decoder
decoder_configs:
  beam_width: 10
  # corresponds to [SIL]
  blank_id: 0

# scheduler: cosine annealing w/ warmups only
scheduler_manual:
  use: true
  configs:
    min_lr: 0.001
    stages: 3
    warmup_epochs: 0
  

# pytorch scheduler
scheduler:
  use: false
  # # sample configs for MultiStepLR
  # choice: multi-step
  # configs:
  #   milestones:
  #     - 8
  #     - 12
  #     - 16
  #   gamma: 0.5
  # sample configs for ReduceLROnPlateau
  choice: reduce-on-plateau
  configs:
    mode: min
    factor: 0.5
    patience: 4
    threshold: 0.5
    threshold_mode: abs
  # for reduce-on-plateau only: track dist or loss
  use_dist_plateau: true
  
# optimizer
optimizer:
  name: adamw
  configs:
    lr: 0.004


# FOR INFERENCE
exp_folder: experiments/<subfolder>

test_decoder_configs:
  beam_width: 50
  # corresponds to [SIL]
  blank_id: 0

use_min_loss: true
use_min_dist: false
use_last: false